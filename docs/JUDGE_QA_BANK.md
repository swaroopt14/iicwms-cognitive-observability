# Chronos AI (IICWMS) Judge Q&A Bank + End-to-End Flow (Technical + Logic)

Last updated: 2026-02-17

This document is designed for judge/mentor questioning. It includes:
- A step-by-step explanation of how the system works end-to-end.
- The hardest questions judges can ask (including "trap" questions).
- Judge-satisfying answers grounded in the current repo implementation.

Repo anchors (most judge questions map here):
- `docs/architecture.md`
- `docs/JUDGE_RISK_LOGIC_GUIDE.md`
- `api/server.py`
- `api/slack_notifier.py`
- `agents/*.py`
- `rag/query_engine.py`
- `blackboard/state.py`
- `observation/layer.py`
- `guards.py`

---

## 0) One-Minute System Summary (Judge-Friendly)

Chronos AI is a cognitive observability layer that converts raw operational facts into evidence-backed insights.

- **Observe:** Append-only raw events/metrics are ingested and stored as facts (no intelligence at ingest).
- **Detect:** Deterministic agents detect workflow anomalies, resource anomalies, policy violations, and baseline deviations.
- **Forecast:** A deterministic risk forecast projects risk trajectory (NORMAL→…→INCIDENT) with explicit confidence formulas.
- **Reason:** A causal agent links cause-effect patterns using temporal proximity + known dependency patterns (no ML required).
- **Explain:** An explainer produces human-readable insight; LLMs (optional) are restricted to wording only (no detection/decisions).
- **Act:** The system emits recommendations and optionally posts high-priority alerts to Slack with cooldown + strict de-dup.
- **Learn (limited):** Rolling baselines adapt thresholds over time; the coordinator adapts system pulse (windows/parallelism).

Key judge guarantee: **All detection and risk math is deterministic.** LLMs are not used for detection and are prevented from writing state.

---

## 1) End-to-End Loop: Observe -> Detect -> Forecast -> Reason -> Explain -> Act -> Learn

### 1.1 Observe (Raw Facts Only)

What happens:
- The system ingests raw telemetry as either:
  - a normalized enterprise envelope (`POST /ingest/envelope`), or
  - a GitHub webhook (`POST /ingest/github/webhook`), or
  - demo-mode simulated events/metrics generated by the simulator.

Hard rules (why judges should trust it):
- **Events are facts**: no severity/risk/anomaly labels are allowed at observation time.
- In `api/server.py`, envelope ingest includes:
  - **schema version gate** (must start with `v1`)
  - **idempotency gate** (duplicate keys go to quarantine/DLQ)
  - **time skew gate** (late/future beyond 24h goes to DLQ)
  - **category payload gate** (e.g., infrastructure requires metrics payload)

Outputs:
- `ObservationLayer.observe_event(...)` stores `ObservedEvent`
- `ObservationLayer.observe_metric(...)` stores `ObservedMetric`

Where in code:
- Ingest + validation: `api/server.py` (`/ingest/envelope`, `/ingest/github/webhook`)
- Storage/query: `observation/layer.py`
- Architectural guard for pure events: `guards.py` (`validate_event_has_no_severity`)

Judge framing:
- "We do not ingest 'alerts'. We ingest facts with context. Intelligence happens later."

---

### 1.2 Detect (Deterministic Detection Agents)

What happens:
The coordinator (MasterAgent) starts a reasoning cycle and runs detection agents.

Phase 1 (parallel detection in `agents/master_agent.py`):
- `WorkflowAgent`: detects `WORKFLOW_DELAY`, `MISSING_STEP`, `SEQUENCE_VIOLATION`
  - SLA check and step order check are deterministic.
- `ResourceAgent`: detects `SUSTAINED_RESOURCE_WARNING`, `SUSTAINED_RESOURCE_CRITICAL`, `RESOURCE_DRIFT`
  - requires sustained window (3 readings), and drift uses slope calculation.
- `ComplianceAgent`: evaluates 5 static policies and emits `PolicyHit` (silent violations)
  - examples: after-hours write, unusual location, sensitive access without workflow, service account writes, skipped approval.
- `AdaptiveBaselineAgent`: maintains rolling mean/stddev and flags `BASELINE_DEVIATION` when sigma deviation exceeds threshold.
- `CodeAgent` (pre-deploy signal): turns ingested GitHub/CI signals into predictive anomalies:
  - `HIGH_CHURN_PR`, `LOW_TEST_COVERAGE`, `HIGH_COMPLEXITY_HINT`, `HOTSPOT_FILE_CHANGE`

Why this is judge-defensible:
- Every anomaly includes `evidence` (event IDs / metric evidence keys) and a deterministic confidence number.
- No LLM is involved in detection.

Where in code:
- Orchestration: `agents/master_agent.py`
- Detection: `agents/workflow_agent.py`, `agents/resource_agent.py`, `agents/compliance_agent.py`,
  `agents/adaptive_baseline_agent.py`, `agents/code_agent.py`
- Evidence enforcement: `guards.py` (`validate_anomaly_has_evidence`)

Judge framing:
- "Detection is rule/statistics-based. LLMs are optional and only used later for explanation wording."

---

### 1.3 Forecast (Risk Trajectory + System Risk Index)

There are **two** complementary risk outputs:

1) **Trajectory forecast (entity-level)**: `RiskForecastAgent`
- Inputs: anomalies + policy hits
- Maintains per-entity counters:
  - `anomaly_count`
  - `policy_violation_count`
- Projects state using explicit mapping:
  - `total_issues = anomaly_count + 2*policy_violation_count`
  - `0 -> NORMAL`
  - `<=1 -> DEGRADED`
  - `<=3 -> AT_RISK`
  - `<=5 -> VIOLATION`
  - `else -> INCIDENT`
- Confidence formula (explicit):
  - `confidence = min(0.95, 0.5 + min(0.3, anomaly_count*0.1) + min(0.2, policy_violation_count*0.1))`
- Emits `RiskSignal` only when risk is **escalating**.

2) **System risk index (cycle-level, 0-100)**: `RiskIndexTracker`
- Inputs: completed cycle anomalies/policy hits/risk signals
- Computes weighted score:
  - `risk_score = 0.35*workflow_risk + 0.35*resource_risk + 0.30*compliance_risk`
- Persists contributions with evidence IDs (for auditability).

Where in code:
- Forecast logic: `agents/risk_forecast_agent.py`
- Risk index: `metrics/risk_index.py`
- Background recording after each cycle: `api/server.py` reasoning loop
- Risk details doc: `docs/JUDGE_RISK_LOGIC_GUIDE.md`

Judge framing:
- "Forecast is not magic. It's deterministic, evidence-driven risk trajectory with explicit formulas."

---

### 1.4 Reason (Causal Links + Severity + Recommendations)

Reasoning adds structure to "what happened" and "what to do next" without hallucination.

1) **CausalAgent** (cause-effect chain)
- Inputs: anomalies + policy hits + risk signals
- Candidate links are created when signals happen within ~60 seconds and match known patterns.
- Confidence starts from a known pattern and is reduced by time distance.
- Optional: writes to Neo4j if enabled (future scope).

2) **SeverityEngineAgent** (0-10)
- Deterministic severity translator:
  - base score computed from anomaly type + anomaly confidence
  - context factors (asset/data/time/role/repetition/blast/module) apply a bounded weighted delta
  - label mapping: `None/Low/Medium/High/Critical`

3) **RecommendationEngineAgent** (structured actions)
- Rules map issue types + severity to specific actions.
- Confidence is deterministic:
  - `conf = 0.5*base_rule_conf + 0.2*(severity/10) + 0.3*(context_match)`

Where in code:
- Causal: `agents/causal_agent.py`
- Severity: `agents/severity_engine_agent.py`
- Structured recommendations: `agents/recommendation_engine_agent.py`
- Baseline rule mapping in coordinator (legacy map): `agents/master_agent.py` (`SOLUTION_MAP`)

Judge framing:
- "We do not invent actions. We map evidence -> issue -> severity -> rule -> recommendation."

---

### 1.5 Explain (Human Insight, LLM Optional)

What happens:
- `ExplanationEngine.generate_insight(cycle)` creates an `Insight` when there is something meaningful to report.
- Explanation path is:
  1) CrewAI (optional) > 2) Gemini LLM (optional) > 3) deterministic templates (default)
- Even when LLM is enabled:
  - LLM is only used for wording/summary, not detection.
  - The underlying anomalies/policy hits/risk signals are deterministic inputs.

Outputs:
- `Insight`: summary, why it matters, what happens if ignored, recommended actions, confidence, severity, evidence count.

Where in code:
- `explanation/engine.py`
- Guard intent: `guards.py` (`llm_cannot_write_state`, `validate_insight_has_evidence`)

Judge framing:
- "LLM can help with language, but it cannot change state or create detections."

---

### 1.6 Act (Operator Outputs: UI + Slack + APIs)

What happens:
- UI reads the API to show anomalies, risk, causal chains, insights.
- Optional Slack integration posts alerts when thresholds are crossed.

Slack logic (judge-friendly):
- Slack alert triggers if **either** threshold is crossed:
  - severity >= configured minimum, OR
  - risk state >= configured minimum
- Anti-noise mechanisms:
  - cooldown (e.g., 120 seconds)
  - strict de-dup fingerprint: same alert content is never sent twice
  - cycle ID de-dup: same cycle never re-alerts

Where in code:
- Slack: `api/slack_notifier.py`
- Called after each cycle: `api/server.py` reasoning loop
- Config: `api/config.py` (ENABLE_SLACK_ALERTS, thresholds, cooldown)

Judge framing:
- "This is decision support: we alert and recommend. We do not auto-remediate."

---

### 1.7 Learn (What “Learning” Means Here)

This build implements **limited, explainable adaptation**, not ML training.

1) Adaptive baselines (rolling statistics):
- Baselines evolve continuously using windowed mean/stddev with smoothing (`ADAPTATION_RATE`).
- This reduces false positives when a metric’s normal shifts over time.
- Also generates evidence-backed deviations when current value is multiple sigmas from learned normal.

2) Coordinator adaptation (system pulse):
- MasterAgent tracks system pulse and adapts:
  - observation window size (events/metrics pulled per cycle)
  - worker pool size (parallelism)
- It also stores cycle diagnostics for trend detection across cycles.

Where in code:
- Baselines: `agents/adaptive_baseline_agent.py`
- Pulse & orchestration: `agents/master_agent.py`
- Scope limitations: `docs/assumptions.md` (no production-grade long-horizon ML learning in Phase-1)

Judge framing:
- "We use simple, auditable adaptation: rolling baselines and adaptive orchestration, not opaque ML."

---

## 2) Prediction Engine: What Exactly Is “Prediction” Here?

Judges often ask this because teams overclaim prediction.

Chronos does **two kinds of prediction**, both deterministic:

### 2.1 Pre-Deploy Prediction (CodeAgent)

What it predicts:
- "This release is risky" before runtime failure is observed, based on code/CI signals.

How it works (deterministic heuristics on already ingested webhook payloads):
- High churn PR: churn >= 40 lines -> `HIGH_CHURN_PR`
- Low test coverage: coverage < 0.70 -> `LOW_TEST_COVERAGE`
- Complexity hint: if provided and >= 8 -> `HIGH_COMPLEXITY_HINT`
- Hotspot file change: filenames/title contain regex/auth/policy/payment -> `HOTSPOT_FILE_CHANGE`

Why it is credible:
- Uses evidence from the same observation layer (no external GitHub calls).
- Emits an anomaly with evidence IDs and confidence.

Where:
- `agents/code_agent.py`
- Ingestion: `api/server.py` (`/ingest/github/webhook`)

### 2.2 Runtime Forecast (RiskForecastAgent)

What it predicts:
- Risk trajectory of an entity based on observed detections and policy hits.

How it works:
- Counters + explicit state mapping + explicit confidence formula.
- Emits signal only on escalation (reduces noise).

Where:
- `agents/risk_forecast_agent.py`
- Risk logic doc: `docs/JUDGE_RISK_LOGIC_GUIDE.md`

Key judge sentence:
- "We forecast trajectory, not exact failure time. The confidence is evidence density, not LLM probability."

---

## 3) Security, Access, and "Too Much Access" (Judge Questions You Must Answer)

### 3.1 "Is it good to use too much access?"

Judge-satisfying answer:
- No. In production, Chronos should run with **least privilege**:
  - Read-only access to telemetry sources where possible.
  - Strictly scoped write permissions only to its own storage (SQLite/queues) and alert channels (Slack webhook).
  - Separate ingest tokens by tenant/project/environment (so one tenant cannot poison another tenant’s state).
  - Rotate secrets and keep webhook URLs as secrets.

What this hackathon build implements vs defers:
- Implemented: guardrails to prevent LLM writing state and to keep observation facts pure (`guards.py`), idempotency + quarantine DLQ on ingest (`api/server.py`).
- Deferred (explicit): production authN/authZ (API keys/OAuth/RBAC), GitHub signature verification, full multi-tenant isolation enforcement.

### 3.2 GitHub Webhook Security

Current build note (be honest):
- `api/server.py` explicitly states signature verification (`X-Hub-Signature-256`) is not implemented in this hackathon build.

Production answer:
- Validate signatures, enforce allowlist of repos/orgs, rate limit, and require auth on public endpoints.

### 3.3 PII / Data Classification

What exists in the envelope:
- `data_classification` and `pii_present` fields in `UnifiedTelemetryEnvelope`.

Production answer:
- Use classification to redact/limit storage and to block sending sensitive content to Slack/LLM.

---

## 4) Judge Q&A Bank (Easy -> Hard)

Use this as rapid-fire practice. Each question has a short, judge-satisfying answer.

### 4.1 Easy (Foundational)

Q: What problem are you solving?
A: Alert fatigue and reactive monitoring. We convert facts into evidence-backed reasoning: Observe -> Detect -> Forecast -> Reason -> Explain -> Act.

Q: What makes this "cognitive observability" and not a dashboard?
A: Dashboards display metrics; Chronos adds deterministic reasoning artifacts (anomalies, policy hits, risk trajectory, causal links, recommendations) with evidence chains.

Q: What are your core layers?
A: Simulator generates reality, Observation stores facts, Agents reason deterministically, Explanation turns artifacts into human insight, API/UI present and alert.

Q: Why do agents not talk directly?
A: To keep reasoning auditable. All communication goes through the Blackboard (`SharedState`), so every step is inspectable and replayable.

Q: What is a reasoning cycle?
A: One loop where we pull a window of observations, run detection agents, then forecast risk, then causal reasoning, then score severity and generate recommendations, then persist the cycle.

---

### 4.2 Medium (Architecture + Logic)

Q: Define "event" vs "anomaly" vs "policy hit" vs "risk signal".
A:
- Event: raw fact (who/what/when), no intelligence.
- Anomaly: detected pattern over events/metrics with confidence and evidence.
- PolicyHit: rule violation against static compliance policies, evidence is the triggering event.
- RiskSignal: forecast of risk trajectory for an entity (current -> projected) with confidence and time horizon.

Q: How do you reduce false positives on resource spikes?
A: ResourceAgent requires sustained breach across multiple readings and uses drift slope for slow degradation. AdaptiveBaselineAgent learns baseline and flags deviations by sigma.

Q: What exactly is the risk forecast math?
A: Per entity: `total_issues = anomalies + 2*policy_hits`, map to NORMAL/DEGRADED/AT_RISK/VIOLATION/INCIDENT, confidence increases with evidence counts and is capped at 0.95.

Q: How does "Ask Chronos" avoid being a chatbot?
A: It decomposes the query into a deterministic type, retrieves evidence from recent cycles, synthesizes an answer from state, and outputs supporting evidence IDs + a confidence computed from evidence confidence (top 10 + small volume bonus).

Q: Where is "dynamic adaptation" implemented?
A:
- AdaptiveBaselineAgent adapts thresholds via rolling windows + smoothing.
- MasterAgent adapts pulse and changes observation windows and parallelism.

---

### 4.3 Hard (Prediction, Slack, Auditability, Safety)

Q: How do you predict before deploy?
A: CodeAgent converts GitHub PR/CI webhook payloads into deterministic pre-deploy anomalies like low test coverage and high churn. Those anomalies become evidence in the same Blackboard cycles as runtime signals.

Q: How do you avoid "LLM hallucinated detection"?
A: LLM is not used in detection agents. ExplanationEngine receives only deterministic artifacts. Guardrails enforce that LLM/explanation code cannot write state (`llm_cannot_write_state`), and insights/anomalies must carry evidence IDs.

Q: How does Slack alerting work, and how do you prevent spam?
A: SlackNotifier alerts if severity OR risk crosses configured thresholds. It has cooldown, strict de-dup fingerprinting (same alert content never repeats), and never alerts twice for the same cycle.

Q: How is the system auditable?
A: Every artifact includes evidence IDs: anomalies reference events/metric evidence keys, policy hits reference event IDs, risk signals reference evidence IDs, causal links reference paired evidence IDs, insights include evidence count and map to a cycle.

Q: What happens if the same telemetry is sent twice?
A: The envelope ingest uses `idempotency_key`. Duplicates are quarantined to a bounded DLQ, and the request is returned as `quarantined` with reason `DUPLICATE`.

---

### 4.4 Trap Questions (Answer Honestly, Then Reframe)

Q: "Is your system multi-tenant secure right now?"
A: The envelope includes a `tenant_key` for organization/project/environment, but full tenant isolation enforcement is future scope for Phase-1. For production we'd partition state/storage by tenant and enforce authN/authZ per tenant.

Q: "Is GitHub webhook signature verification implemented?"
A: Not in this hackathon build; it is explicitly called out in the endpoint docstring. In production we would verify `X-Hub-Signature-256`, enforce allowlists, and not expose the endpoint publicly.

Q: "Do your risk counters ever decay? Won't risk only go up?"
A: Current RiskForecastAgent tracks counts in-memory and signals on escalation; decay/reset is a reasonable next-step for long-running environments. For the demo, it showcases deterministic escalation logic clearly.

Q: "AdaptiveBaselineAgent claims it doesn't detect anomalies, but it emits BASELINE_DEVIATION anomalies."
A: Good catch. The intent is that it detects deviations from learned baseline (a specialized anomaly type) and also exposes baselines for consumption; in future we can split 'baseline computation' from 'baseline deviation detection' if we want stricter separation.

---

### 4.5 Additional Questions Judges Often Ask (Security, Reliability, Scale)

Q: How do you prevent someone from sending fake telemetry to manipulate the system?
A: In production we would require authentication on ingest endpoints, validate signatures for webhooks (GitHub), enforce per-tenant allowlists, and quarantine invalid schemas/timestamps. In this build we already quarantine unsupported schemas, late/future events, and duplicate idempotency keys, and we rate limit requests at the API layer.

Q: Do you store sensitive data or send it to Slack/LLM?
A: The enterprise envelope supports `data_classification` and `pii_present`. In production, those would gate redaction and whether content can be sent to Slack or LLM. In this build, Slack messages summarize cycle-level signals rather than raw payloads, and LLM usage is optional and limited to explanation wording.

Q: What if one agent crashes? Does the whole system stop?
A: The reasoning loop catches cycle errors and retries on the next interval. Each cycle is bounded and the server continues running. (This is visible in the background loop error handling.)

Q: How do you make sure state doesn't grow forever?
A: We bound multiple buffers in memory: insights buffer, cycle history, idempotency-key memory, and DLQ size. Persistence is to SQLite and append-only JSONL backups.

Q: How do you handle out-of-order events?
A: For envelope ingest, we apply a time-skew gate (24 hours). For workflow detection, WorkflowAgent sorts events by timestamp before processing to maintain ordering.

Q: What is your consistency model when agents run in parallel?
A: Agents append outputs to `SharedState` within a cycle; the model is append-only (no overwrites/deletes in-cycle). Concurrency safety is provided by locks inside shared state mutation methods.

Q: Why do you have LangGraph if you also have deterministic execution?
A: LangGraph is an optional orchestrator. When enabled, it executes named nodes; when unavailable or failing, every agent falls back to the same deterministic sequential functions. This provides resilience and prevents orchestration from becoming a single point of failure.

Q: What prevents "too much access" inside the system itself (agents doing unsafe things)?
A: Architectural guardrails define what each layer can do: simulation can emit events but cannot read policies; agents can read observations and write reasoning artifacts but cannot emit events; the LLM/explainer can generate text but cannot write state.

---

## 5) Quick Demo-Defense: What Judges Should Verify Live

1. Ingest a GitHub Actions payload with `test_coverage=0.62`, run a cycle, show CodeAgent emits `LOW_TEST_COVERAGE`.
2. Run cycles until ResourceAgent produces `SUSTAINED_RESOURCE_CRITICAL` (sustained window behavior).
3. Show ComplianceAgent emits `NO_SKIP_APPROVAL` when a `WORKFLOW_STEP_SKIP` with approval is observed.
4. Show RiskForecastAgent escalates state with explicit reasoning and confidence.
5. Show CausalAgent links resource->workflow delay when temporal proximity matches.
6. Show Insight contains evidence count and recommendations.
7. If Slack enabled, show `/alerts/slack/status` and that cooldown/de-dup prevents spam.
